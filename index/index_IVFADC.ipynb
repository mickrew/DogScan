{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"index_IVFADC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OWoDMc02xeBw"},"outputs":[],"source":["import time\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from scipy.spatial.distance import cdist\n","from scipy.cluster.vq import kmeans2\n","import pandas as pd"]},{"cell_type":"markdown","source":["##Coarse Quantizer\n","The coarse quantizer is for non-exhaustive search. It retrieves a candidate set first, then searches within the candidate set for nearest neighbors based on PQ.\n","\n","split the space into k clusters of vectors\n","• usually clusters are defined by k-means\n","• assign vectors to nearest centroid\n","• index = inverted list structure\n","• maps centroid id -> list of vectors assigned to it\n","\n","Coarse-quantization: Given a query vector y ∈\n","RD, the nearest bucket Xj is selected. The residual\n","y−μj between the query and the representative vector\n","of the jth bucket is computed.\n","\n","\n","K coarse centers ¯C = {¯ck }K\n","k=1\n","are created by running the\n","clustering algorithm [35] on ¯X (or its subset). Note that each coarse\n","center is also a PQ-code ¯ck ∈ {1, . . . ,Z}M. Using these coarse\n","centers, the database PQ-codes ¯X are clustered into K groups. The\n","resulting assignments are stored as posting listsW = {Wk }K\n","k=1\n",",\n","where eachWk is a set of identifiers of the database vectors whose\n","nearest coarse center is the kth one:"],"metadata":{"id":"QrpHk3r5x3xj"}},{"cell_type":"code","source":["class KmeansCoarseQuantizer:\n","\n","    def __init__(self, dimension, n_list):\n","        self.dimension = dimension\n","        self.n_list = n_list\n","        self.coarse_quantizer = KMeans(n_clusters=n_list)\n","\n","    def fit(self, train):\n","        model = self.coarse_quantizer.fit(train)\n","        self.centroids = model.cluster_centers_\n","        self.model = model\n","        self.labels = model.predict(train)\n","\n","    def add(self, vecs):\n","      self.labels = self.model.predict(vecs)\n","\n","    def get_labels(self):\n","        return self.labels\n","\n","    def get_centroids(self):\n","        return self.centroids"],"metadata":{"id":"yNBlXllEx3Cn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IVFADC:\n","\n","    def __init__(self, inverted_file_list, dimensions, number_subvectors, n_bits):\n","          self.inverted_file_list = inverted_file_list\n","          self.number_subvectors = number_subvectors\n","          self.number_subspaces = 2 ** n_bits\n","          self.d = dimensions\n","\n","    def train(self, train_vectors, sub_size='all'):\n","      #sub_size specify the number of random element chosen from the entire training set, or the indexes to perform the coarse quantizaion\n","        start_time = time.time()\n","        \n","        if sub_size == 'all':\n","          self.inverted_file_list.fit(train_vectors)\n","        elif type(sub_size) is np.ndarray:\n","          self.inverted_file_list.fit(train_vectors[sub_size])\n","        else:\n","          train_subset_index = np.random.choice(train_vectors.shape[0], sub_size, replace=False)\n","          self.inverted_file_list.fit(train_vectors[train_subset_index])\n","        \n","        self.inverted_file_list.add(train_vectors)\n","\n","        codebooks = []\n","        index = []\n","        for i in range(len(self.inverted_file_list.get_centroids())):\n","            points_id = np.where(self.inverted_file_list.get_labels() == i)[0]\n","            residuals = np.subtract(train_vectors[points_id], self.inverted_file_list.get_centroids()[i])  # compute the residuals\n","            codewords, codebook = self.productQuantization(residuals)\n","            codebooks.append(codebook)\n","            index.append([points_id, codewords])\n","\n","        train_time = time.time() - start_time\n","        self.train_time = train_time\n","        self.codebooks = codebooks\n","        self.index = index\n","    \n","    def get_train_time(self):\n","      return self.train_time\n","\n","    def productQuantization(self, vecs):\n","        M = self.number_subvectors\n","        Ks = self.number_subspaces\n","        assert vecs.ndim == 2\n","        N, D = vecs.shape\n","        Ds = int(D / M)\n","\n","        codebook = np.zeros((Ks, D))\n","        subquantizers = np.zeros((N, M), dtype=np.uint8)  # 8 bit\n","\n","        for i, vecs_sub in enumerate(np.split(vecs, M, axis=1)):\n","            centroids, labels = kmeans2(vecs_sub, Ks, minit='points')\n","            codebook[:, i * Ds: (i + 1) * Ds] = centroids\n","            subquantizers[:, i] = labels\n","        return subquantizers, codebook\n","\n","    def search(self, q, n_neighbours, n_probe, metric):\n","\n","        assert self.d % self.number_subvectors == 0\n","        start = time.time()\n","\n","        dim_subvector = int(self.d / self.number_subvectors)\n","\n","        self.n_probe = n_probe\n","\n","        distances_matrix_queries = cdist(q, self.inverted_file_list.get_centroids(), metric='sqeuclidean')  # compute multidimensional distances\n","        clusters_ID = distances_matrix_queries.argsort(axis=1)[:, :n_probe]\n","\n","        # Compute nearest images from queries\n","        neighbours = []\n","        neighbours_distances = []\n","\n","        for i, query in enumerate(clusters_ID):\n","            Similarities_q = []  # keep all the ... between the query and the kNN in each accessed cluster\n","            point_ID = []\n","            for centroid_id in query:\n","                entry = self.index[centroid_id]\n","                query_residual = np.subtract(q[i], self.inverted_file_list.get_centroids()[centroid_id])\n","\n","                # summing up the contribution distance of each sub_vector\n","                query_sim = np.zeros((1, entry[1].shape[0]))\n","                for m, vecs_sub in enumerate(np.split(entry[1], self.number_subvectors, axis=1)):\n","                    sub_query = query_residual[m * dim_subvector:(m + 1) * dim_subvector].reshape(1, -1)\n","                    # reconstruct the apprisimated vector using centroids' ID: codebooks[centroid_id][vecs_sub.ravel(), m * dim_subvector:(m + 1) * dim_subvector]\n","                    if metric == 'sqeuclidean':\n","                      sim_query_sub_centroids = cdist(sub_query,\n","                                                      self.codebooks[centroid_id][:, m * dim_subvector:(m + 1) * dim_subvector],\n","                                                      metric='sqeuclidean')\n","                      \n","                    elif metric == 'cosine':\n","                      sim_query_sub_centroids = cdist(sub_query, \n","                                                      self.codebooks[centroid_id][:, m * dim_subvector:(m + 1) * dim_subvector], \n","                                                      metric='cosine')\n","                      sim_query_sub_centroids = np.subtract(np.ones(sim_query_sub_centroids.shape), sim_query_sub_centroids)\n","\n","                    elif metric == 'dot':\n","                      sim_query_sub_centroids = sub_query.dot(self.codebooks[centroid_id][:, m * dim_subvector:(m + 1) * dim_subvector].T)\n","                      \n","                    else:\n","                      print(\"Metric not allowed!\")\n","                      exit()\n","                    \n","                    sim_contributions = sim_query_sub_centroids[:, vecs_sub.ravel()]\n","                    query_sim = np.sum([sim_contributions, query_sim], axis=0)\n","                \n","                Similarities_q += query_sim[0].tolist()\n","                point_ID += entry[0].tolist()\n","\n","            if metric == 'sqeuclidean':\n","              indexes = np.argsort(np.array(Similarities_q))[:n_neighbours]\n","            elif metric == 'cosine' or metric == 'dot':\n","              indexes = np.argsort(np.array(Similarities_q))[::-1][:n_neighbours]\n","\n","            neighbours.append(np.array(point_ID)[indexes])\n","            neighbours_distances.append(np.array(Similarities_q)[indexes])\n","\n","            search_time = time.time() - start\n","            dist_computed = n_probe*self.number_subspaces\n","        return np.array(neighbours), np.array(neighbours_distances), search_time\n","\n","    def compute_recall(self, true_neighbors, predicted_neighbors):\n","        recalls = []\n","        for t, p in zip(true_neighbors, predicted_neighbors):\n","            intersection = np.intersect1d(t, p)  # find common elements\n","            recall = len(intersection) / len(t)\n","            recalls.append(recall)\n","\n","        return np.mean(recalls)\n","\n","    def save_index(self, path_index):\n","      with open(path_index, 'wb') as f:\n","        pickle.dump(self, f)\n","      return"],"metadata":{"id":"N9iStDowx_du"},"execution_count":null,"outputs":[]}]}